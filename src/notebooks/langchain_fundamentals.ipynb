{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f2b328",
   "metadata": {},
   "source": [
    "# üìò Lesson 1 ‚Äî LangChain Fundamentals for Agentic Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85392cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b467ce1",
   "metadata": {},
   "source": [
    "Before we can build an autonomous agent that can **reason** and **act**, we must first understand the basic tools used to build it.\n",
    "\n",
    "LangChain does **not** work because of one magical `\"agent\"` function.\n",
    "Its real power comes from a simple but strong idea:\n",
    "\n",
    "> **Composability**\n",
    "\n",
    "This lesson is fully dedicated to this core principle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36516203",
   "metadata": {},
   "source": [
    "\n",
    "## üîπ Core Philosophy: Composability and LCEL\n",
    "\n",
    "LangChain is built around small components that can be **combined together**.\n",
    "Each component does **one job**, and we connect them step by step.\n",
    "\n",
    "This idea is inspired by pipelines:\n",
    "\n",
    "* input goes in,\n",
    "* transformations happen,\n",
    "* output comes out.\n",
    "\n",
    "LangChain formalizes this idea with the **LangChain Expression Language (LCEL)**.\n",
    "\n",
    "\n",
    "### üß± The Three Core Components\n",
    "\n",
    "In most LangChain applications, you will work with these three elements:\n",
    "\n",
    "* **Prompt** :\n",
    "  Defines *what* we ask the model.\n",
    "\n",
    "* **Model** :\n",
    "  The language model that generates the response.\n",
    "\n",
    "* **Output Parser** :\n",
    "  Transforms the raw model output into a usable format.\n",
    "\n",
    "A very simple chain looks like this:\n",
    "\n",
    "```\n",
    "Prompt | Model | Output Parser\n",
    "```\n",
    "\n",
    "This is the **foundation of agentic systems**.\n",
    "Even complex agents are built by composing simple chains like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f67f8",
   "metadata": {},
   "source": [
    "### üîπ Building Our First Chain with LCEL\n",
    "\n",
    "Let‚Äôs now build our **first LangChain pipeline** to see composability in action.\n",
    "\n",
    "The goal is simple:\n",
    "\n",
    "> Generate a short and funny tagline for a company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9766b2",
   "metadata": {},
   "source": [
    "> ‚úçÔ∏è Step 1 ‚Äî Define the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed546c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "promp_template = ChatPromptTemplate.from_template(\"Generate a short, funny tagline for a company that makes: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da473038",
   "metadata": {},
   "source": [
    "This prompt contains a **variable** called `{topic}`.\n",
    "It allows us to reuse the same prompt with different inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcf5bf",
   "metadata": {},
   "source": [
    "\n",
    "> üß† Step 2 ‚Äî Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07bee149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  ChatOpenAI(model=\"gpt-4o\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2067e",
   "metadata": {},
   "source": [
    "\n",
    "* We use `gpt-4o`\n",
    "* `temperature=0` means:\n",
    "\n",
    "  * more stable\n",
    "  * more deterministic\n",
    "  * good for reproducible behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85833901",
   "metadata": {},
   "source": [
    "> üîÑ Step 3 ‚Äî Define the Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e44d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532791e",
   "metadata": {},
   "source": [
    "The model returns a complex object.\n",
    "The `StrOutputParser` extracts **only the text**, as a simple string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6970b",
   "metadata": {},
   "source": [
    "> üîó Step 4 ‚Äî Compose the Chain with LCEL\n",
    "\n",
    "Here, we define a processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e32f43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = promp_template | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd409f12",
   "metadata": {},
   "source": [
    "- The prompt formats the input data.\n",
    "\n",
    "- The language model generates a response from that prompt.\n",
    "\n",
    "- The output parser transforms the model response into a simple string.\n",
    "\n",
    "Each component performs a single transformation.\n",
    "LCEL makes these transformations explicit and ordered.\n",
    "\n",
    "This way of composing logic is central to LangChain:\n",
    "\n",
    "- behavior is built by composition, not by inheritance,\n",
    "\n",
    "- complex systems emerge from simple, connected blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b2716",
   "metadata": {},
   "source": [
    "> ‚ñ∂Ô∏è Step 5 ‚Äî Invoke the Chain\n",
    "\n",
    "Here:\n",
    "\n",
    "* we pass the input as a dictionary\n",
    "* the chain runs from start to end\n",
    "* we receive a clean string as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ef5cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Topic: Agentic AI\n",
      "Generated Tagline: \"Agentic AI: Because even your to-do list deserves a personal assistant with a sense of humor!\"\n"
     ]
    }
   ],
   "source": [
    "topic_input = {\"topic\": \"Agentic AI\"}\n",
    "result = chain.invoke(topic_input)\n",
    "print(f\"Input Topic: {topic_input['topic']}\")\n",
    "print(f\"Generated Tagline: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7dc4a",
   "metadata": {},
   "source": [
    "At this stage, we did **not** build an agent yet.\n",
    "\n",
    "But we learned something more important:\n",
    "\n",
    "* LangChain is **modular**\n",
    "* Every agent is built from **simple chains**\n",
    "* LCEL makes data flow **explicit and readable**\n",
    "\n",
    "This pattern:\n",
    "\n",
    "```\n",
    "Prompt | Model | Output Parser\n",
    "```\n",
    "\n",
    "will appear again and again when building agentic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd0ef4",
   "metadata": {},
   "source": [
    "### Practical LCEL ‚Äî Piping Components Together for Inputs and Outputs\n",
    "\n",
    "In the previous section, the chain was simple:  \n",
    "one input variable ‚Üí one prompt ‚Üí one output.\n",
    "\n",
    "Real-world applications are rarely that simple.\n",
    "\n",
    "Very often, a chain must work with **several pieces of information at the same time**.\n",
    "For example:\n",
    "- a name and a description,\n",
    "- a question and some context,\n",
    "- user input and retrieved data.\n",
    "\n",
    "LCEL supports this kind of data flow through objects called **Runnables**.\n",
    "Runnables allow us to **control how information moves and transforms** inside a chain.\n",
    "\n",
    "In this section, we focus on two common Runnable patterns:\n",
    "- `RunnablePassthrough`\n",
    "- `RunnableParallel`\n",
    "\n",
    "We start with the most basic one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d71f86",
   "metadata": {},
   "source": [
    "### Handling Multiple Inputs with RunnablePassthrough\n",
    "\n",
    "The goal is simple:\n",
    "\n",
    "> Take one input dictionary and make its values available to the prompt in a clean and explicit way.\n",
    "\n",
    "We will build a small preparation step that passes data forward without modifying its meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3abaf",
   "metadata": {},
   "source": [
    "#### Step 1 ‚Äî Define the Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf5b4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.5)\n",
    "promp_template = ChatPromptTemplate.from_template(\n",
    "    \"Write a short biography about {name}. \"\n",
    "    \"Focus on the following contribution: {context}. \"\n",
    "    \"Keep it light and readable.\"\n",
    ")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96820797",
   "metadata": {},
   "source": [
    "The prompt expects a dictionary like:\n",
    "\n",
    "```python\n",
    "{\"name\": \"...\", \"context\": \"...\"}\n",
    "```\n",
    "In practice, input data can:\n",
    "\n",
    "- come from another chain,\n",
    "\n",
    "- contain extra fields,\n",
    "\n",
    "- or be produced dynamically.\n",
    "\n",
    "So we often need an explicit preparation step to control what the prompt receives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357855f",
   "metadata": {},
   "source": [
    "#### Step 2 ‚Äî Passing Data Forward with RunnablePassthrough Logic\n",
    "\n",
    "The idea behind `RunnablePassthrough` is simple: take the input from the previous step and make it available downstream.\n",
    "\n",
    "Here, we build a small mapping that:\n",
    "- receives the full input dictionary,\n",
    "- extracts the fields we need,\n",
    "- produces a clean dictionary for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "510f35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_and_retrieval = {\n",
    "    \"name\": lambda x: x[\"name\"],\n",
    "    \"context\": lambda x: x[\"context\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b928eb0",
   "metadata": {},
   "source": [
    "This step does not generate new information.\n",
    "It only **selects and forwards** existing values.\n",
    "\n",
    "This pattern appears very often in agentic systems:\n",
    "- passing user input,\n",
    "- passing tool outputs,\n",
    "- passing retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e29e028",
   "metadata": {},
   "source": [
    "#### Step 3 ‚Äî Compose the Full Chain\n",
    "\n",
    "The data flow is now explicit:\n",
    "\n",
    "- the input dictionary enters the setup step,\n",
    "- the setup step prepares the prompt variables,\n",
    "- the prompt formats a message,\n",
    "- the model generates text,\n",
    "- the parser extracts a clean string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faba5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup_and_retrieval | promp_template | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97d3a73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Biography Generation -----\n",
      "Input: {'name': 'Albert Einstein', 'context': 'the theory of relativity'}\n",
      "\n",
      "Generated Biography:\n",
      "Albert Einstein, born on March 14, 1879, in Ulm, Germany, is one of the most celebrated physicists in history, best known for his groundbreaking theory of relativity. As a child, Einstein showed a deep curiosity about the mysteries of the universe, which eventually led him to revolutionize our understanding of space, time, and energy.\n",
      "\n",
      "Einstein's journey to fame began in 1905, a year often referred to as his \"miracle year.\" During this time, he published several pivotal papers, including one that introduced the special theory of relativity. This theory fundamentally changed the way we perceive the fabric of the universe by establishing that the laws of physics are the same for all non-accelerating observers and that the speed of light is constant, regardless of the observer's motion. A key outcome of this theory was the iconic equation \\(E=mc^2\\), which demonstrated the equivalence of mass and energy.\n",
      "\n",
      "In 1915, Einstein expanded his ideas further with the general theory of relativity, which proposed that gravity is not a force but a curvature of spacetime caused by mass. This theory not only provided a new understanding of gravity but also predicted phenomena such as the bending of light around massive objects, which was later confirmed during a solar eclipse in 1919, catapulting Einstein to international fame.\n",
      "\n",
      "Einstein's work on relativity has had a profound impact on modern physics, influencing everything from the development of GPS technology to our understanding of black holes and the expanding universe. Despite these complex ideas, Einstein himself was known for his playful personality and love of music, often seen with his violin in hand.\n",
      "\n",
      "Albert Einstein passed away on April 18, 1955, in Princeton, New Jersey, but his legacy continues to inspire scientists and dreamers alike, reminding us of the boundless possibilities of human curiosity and intellect.\n"
     ]
    }
   ],
   "source": [
    "input_data = {\n",
    "    \"name\": \"Albert Einstein\",\n",
    "    \"context\": \"the theory of relativity\"\n",
    "}\n",
    "\n",
    "result = chain.invoke(input_data)\n",
    "\n",
    "print(\"----- Biography Generation -----\")\n",
    "print(f\"Input: {input_data}\")\n",
    "print(\"\\nGenerated Biography:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a62d9c",
   "metadata": {},
   "source": [
    "> **What to Take Away ?**\n",
    "\n",
    "- The chain does not depend on where the input comes from.\n",
    "- Each step has a clear responsibility.\n",
    "- Data preparation is explicit and readable.\n",
    "\n",
    "This separation becomes essential when building agents:\n",
    "reasoning, tools, and memory all rely on the same idea of controlled data flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308fee8",
   "metadata": {},
   "source": [
    "### Parallel Processing with RunnableParallel\n",
    "\n",
    "Until now, all chains followed a linear path:\n",
    "each component processed the output of the previous one.\n",
    "\n",
    "Agentic systems often need a different structure.\n",
    "They must look at the **same input from several angles at once**.\n",
    "\n",
    "Typical examples include:\n",
    "- producing a summary and an analysis in parallel,\n",
    "- extracting facts while keeping the original context,\n",
    "- generating multiple candidates before making a decision.\n",
    "\n",
    "LCEL supports this pattern through **RunnableParallel**.\n",
    "It allows one input to feed several branches at the same time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b902131",
   "metadata": {},
   "source": [
    "To illustrate this idea, we will reuse a single language model,\n",
    "but ask it to perform **different tasks in parallel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8460be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3baf5",
   "metadata": {},
   "source": [
    "Each branch of the pipeline is defined by its own prompt.\n",
    "All of them will receive the **same input dictionary**,\n",
    "but they will produce different kinds of outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "358406d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a short summary about {topic}.\"\n",
    ")\n",
    "\n",
    "key_points_prompt = ChatPromptTemplate.from_template(\n",
    "    \"List three key points about {topic}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030ae90e",
   "metadata": {},
   "source": [
    "These branches are then combined into a single parallel structure.\n",
    "`RunnableParallel` takes care of sending the input to all branches\n",
    "and collecting their outputs in a structured way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cc5272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel(\n",
    "    summary=summary_prompt | model | output_parser,\n",
    "    key_points=key_points_prompt | model | output_parser,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03fad3b",
   "metadata": {},
   "source": [
    "When the chain is invoked, both branches are executed independently.\n",
    "The result is a dictionary that groups all outputs under clear keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d14770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Parallel Processing Result -----\n",
      "\n",
      "Summary:\n",
      "Agentic AI refers to a branch of artificial intelligence research and development focused on creating systems that exhibit autonomous decision-making and problem-solving capabilities. These AI systems are designed to operate with a degree of independence, making proactive decisions to achieve specific goals or tasks without requiring constant human oversight. Agentic AI can be applied in various domains, from robotics and personal assistants to autonomous vehicles and smart environments. The development of Agentic AI involves challenges such as ensuring ethical behavior, understanding complex environments, and interacting seamlessly with humans and other systems. The ultimate goal is to create AI agents that can adapt to new situations, learn from experiences, and make decisions that align with human values and preferences.\n",
      "\n",
      "Key Points:\n",
      "Agentic AI refers to artificial intelligence systems that exhibit characteristics typically associated with autonomy and agency. Here are three key points about Agentic AI:\n",
      "\n",
      "1. **Autonomy and Decision-Making**: Agentic AI systems can operate with a degree of independence, making decisions based on their programming and the data they process. These systems can assess situations, predict outcomes, and take actions without direct human intervention, resembling aspects of human-like decision-making.\n",
      "\n",
      "2. **Goal-Orientation**: These AI systems are often designed with specific objectives or goals and can initiate actions to achieve those goals. They can adapt their behavior based on feedback from the environment to optimize results, showing a form of goal-directed behavior that resembles intentionality.\n",
      "\n",
      "3. **Ethical and Safety Considerations**: The development and deployment of Agentic AI bring significant ethical and safety challenges. There is a need for rigorous standards to ensure they act in alignment with human values and do not cause unintended harm. This includes considerations of accountability, transparency, and control to prevent potential risks associated with autonomous decision-making systems.\n"
     ]
    }
   ],
   "source": [
    "input_data = {\"topic\": \"Agentic AI\"}\n",
    "result = parallel_chain.invoke(input_data)\n",
    "\n",
    "print(\"----- Parallel Processing Result -----\")\n",
    "print(\"\\nSummary:\")\n",
    "print(result[\"summary\"])\n",
    "\n",
    "print(\"\\nKey Points:\")\n",
    "print(result[\"key_points\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761770ea",
   "metadata": {},
   "source": [
    "The data flow is now easy to reason about:\n",
    "\n",
    "- one input enters the system,\n",
    "- several transformations happen in parallel,\n",
    "- results are gathered without conflict.\n",
    "\n",
    "This structured parallelism is essential for agents.\n",
    "Before acting, an agent may need to:\n",
    "- analyze a situation,\n",
    "- check constraints,\n",
    "- generate possible actions.\n",
    "\n",
    "`RunnableParallel` makes this pattern explicit and manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac82d24",
   "metadata": {},
   "source": [
    "## Integrating Your First LLM (OpenAI)\n",
    "\n",
    "So far, our chains focused on **structure**:\n",
    "how prompts, runnables, and parsers connect together.\n",
    "\n",
    "At the center of all these pipelines sits one critical component:\n",
    "the **model**.\n",
    "\n",
    "The model is the part that performs the actual reasoning.\n",
    "It reads the prompt, interprets the context, and generates a response.\n",
    "\n",
    "### Why the Model Abstraction Matters\n",
    "\n",
    "LangChain provides a **standard interface** to interact with many different language models:\n",
    "- OpenAI\n",
    "- Anthropic\n",
    "- open-source models\n",
    "- self-hosted models\n",
    "\n",
    "This design choice has an important consequence:\n",
    "\n",
    "> You are not locked into a single provider.\n",
    "\n",
    "The rest of your chain (prompts, parsers, runnables) can stay the same\n",
    "even if you change the underlying model later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7239a5c1",
   "metadata": {},
   "source": [
    "### Why Start with OpenAI?\n",
    "\n",
    "Many developers begin with OpenAI models because they:\n",
    "- perform well across many tasks,\n",
    "- follow instructions reliably,\n",
    "- integrate easily into existing workflows.\n",
    "\n",
    "In LangChain, OpenAI chat models are accessed through the `ChatOpenAI` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7558d03",
   "metadata": {},
   "source": [
    "To make things concrete, we will build a **minimal chain** that uses\n",
    "`gpt-4o` as its reasoning engine.\n",
    "\n",
    "The goal is not complexity,\n",
    "but to clearly see how a model fits into the LCEL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c330778",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ac778",
   "metadata": {},
   "source": [
    "The temperature controls how deterministic the output is:\n",
    "- lower values ‚Üí more stable, repeatable answers\n",
    "- higher values ‚Üí more creative, varied answers\n",
    "\n",
    "For learning and debugging, lower values are usually better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d074b7b",
   "metadata": {},
   "source": [
    "### Connecting the Model to a Prompt\n",
    "\n",
    "The model itself does nothing without a prompt.\n",
    "We start with a simple instruction that takes one input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8652ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "promp_template = ChatPromptTemplate.from_template(\"Explain the concept of {topic} in simple terms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073bcd47",
   "metadata": {},
   "source": [
    "The model response is a rich object.\n",
    "\n",
    "To keep things simple, we will extract only the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e00e1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187c0be",
   "metadata": {},
   "source": [
    "With all components defined, we can now connect them into a single chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5aaef3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = promp_template | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177263e7",
   "metadata": {},
   "source": [
    "When the chain is invoked, the input flows through the prompt, then into the model, and finally into the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f31b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Model Output -----\n",
      "Agentic AI refers to artificial intelligence systems that can act autonomously and make decisions on their own to achieve specific goals. These systems are designed to perform tasks without needing constant human intervention. They can perceive their environment, process information, and take actions that help them accomplish their objectives. Essentially, agentic AI can be thought of as AI with a degree of independence, capable of adapting to new situations and making choices based on the data it receives and its programmed goals.\n"
     ]
    }
   ],
   "source": [
    "input_data = {\"topic\": \"agentic AI\"}\n",
    "\n",
    "result = chain.invoke(input_data)\n",
    "\n",
    "print(\"----- Model Output -----\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb5a07",
   "metadata": {},
   "source": [
    "## Understanding Streaming, Batching, and Asynchronous Calls with LCEL\n",
    "\n",
    "So far, we have used `invoke()` to run a chain and get a single result.\n",
    "This works well for simple experiments and demos.\n",
    "\n",
    "Real-world applications have different constraints:\n",
    "- users expect **fast and continuous feedback**,\n",
    "- systems must handle **many requests efficiently**,\n",
    "- backends often need to **run tasks concurrently**.\n",
    "\n",
    "LCEL supports these needs through three execution modes:\n",
    "**streaming**, **batching**, and **asynchronous calls**.\n",
    "\n",
    "Each mode solves a different problem, but all of them use the same chain structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f04e4b",
   "metadata": {},
   "source": [
    "We will reuse a simple chain to focus on *how* it is executed,\n",
    "not on *what* it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19186716",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.4)\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Give a short explanation of {topic}.\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1ded49",
   "metadata": {},
   "source": [
    "### Streaming ‚Äî Real-Time Feedback\n",
    "\n",
    "In interactive applications (chat interfaces, assistants, dashboards),\n",
    "waiting for the full response can feel slow.\n",
    "\n",
    "**Streaming** solves this by returning partial outputs as soon as they are generated.\n",
    "\n",
    "Instead of waiting for the final answer,\n",
    "the user sees the response appear token by token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc0b22f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI refers to artificial intelligence systems that are designed to operate with a degree of autonomy, making decisions and taking actions in pursuit of specific goals. These systems are often characterized by their ability to perceive their environment, reason about the information they gather, and make decisions based on predefined objectives or learned experiences. Agentic AI can be employed in various applications, such as autonomous vehicles, personal assistants, and robotics, where they act as agents that can interact with the world and adapt to changing conditions. The concept emphasizes the AI's capability to act independently, rather than merely following scripted instructions."
     ]
    }
   ],
   "source": [
    "input = {\"topic\": \"Agentic AI\"}\n",
    "for chunk in chain.stream(input):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410ac4b",
   "metadata": {},
   "source": [
    "What happens here:\n",
    "- the chain starts generating text,\n",
    "- each chunk is sent immediately,\n",
    "- the full response is built progressively.\n",
    "\n",
    "Streaming improves **perceived performance**\n",
    "and is especially useful for user-facing agentic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894dc043",
   "metadata": {},
   "source": [
    "### Batching ‚Äî High-Throughput Processing\n",
    "\n",
    "Sometimes the goal is not speed for one request, but efficiency across **many requests**.\n",
    "\n",
    "Batching allows the same chain to process multiple inputs in one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c524f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: agentic AI\n",
      "Output: Agentic AI refers to artificial intelligence systems designed to operate with a degree of autonomy, making decisions and taking actions to achieve specific goals without requiring constant human oversight. These systems are often equipped with the ability to perceive their environment, process information, and adapt to new situations, enabling them to perform tasks or solve problems in dynamic settings. The concept of agentic AI is closely related to the idea of intelligent agents, which are software entities that act on behalf of users to perform tasks such as information retrieval, decision-making, and automation. The development of agentic AI involves ensuring that these systems can operate safely and align with human values and intentions.\n",
      "\n",
      "Topic: retrieval-augmented generation\n",
      "Output: Retrieval-Augmented Generation (RAG) is a technique that combines the strengths of retrieval-based and generation-based models to improve the quality and accuracy of generated content. In RAG, a retrieval component first searches a large corpus of documents to find relevant information based on a given query or prompt. This retrieved information is then used to inform and guide a generative model, which produces the final output. By integrating external knowledge from the retrieval step, RAG can generate more contextually accurate and informative responses, making it particularly useful for tasks like question answering, summarization, and other applications requiring up-to-date or domain-specific information.\n",
      "\n",
      "Topic: LLM orchestration\n",
      "Output: LLM orchestration refers to the process of managing and coordinating interactions between large language models (LLMs) and other components in a software system to achieve specific tasks or workflows. This involves integrating LLMs with various tools, data sources, and APIs to create a seamless and efficient pipeline that can handle complex operations, such as data processing, decision-making, or user interaction. Orchestration ensures that the LLMs are used effectively, optimizing their strengths while mitigating limitations, and often includes handling tasks like input preprocessing, output postprocessing, error handling, and context management to enhance the overall functionality and reliability of AI applications.\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    {\"topic\": \"agentic AI\"},\n",
    "    {\"topic\": \"retrieval-augmented generation\"},\n",
    "    {\"topic\": \"LLM orchestration\"},\n",
    "]\n",
    "\n",
    "results = chain.batch(inputs)\n",
    "\n",
    "for topic, output in zip(inputs, results):\n",
    "    print(f\"\\nTopic: {topic['topic']}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa79c4d",
   "metadata": {},
   "source": [
    "Batching is useful when:\n",
    "- processing large datasets,\n",
    "- running offline jobs,\n",
    "- evaluating prompts at scale.\n",
    "\n",
    "From a conceptual point of view,\n",
    "the chain stays the same ‚Äî only the **execution mode changes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf19e6",
   "metadata": {},
   "source": [
    "### Asynchronous Calls ‚Äî Concurrent Execution\n",
    "\n",
    "Modern applications often handle multiple tasks at the same time:\n",
    "- several users,\n",
    "- multiple agents,\n",
    "- parallel tool calls.\n",
    "\n",
    "LCEL supports asynchronous execution through `ainvoke()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8609949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-agent systems (MAS) are computational systems in which multiple autonomous entities, known as agents, interact or work together to achieve individual or collective goals. Each agent in a MAS can perceive its environment, make decisions, and perform actions based on its own objectives and the information it gathers. These systems are often used to solve complex problems that are difficult or impossible for a single agent to tackle alone. Agents in a MAS can collaborate, compete, or coexist, and they may have varying degrees of independence and capabilities. Applications of multi-agent systems include robotics, distributed control systems, simulation, and artificial intelligence, where they can model and manage decentralized processes, enhance scalability, and improve robustness.\n"
     ]
    }
   ],
   "source": [
    "result = await chain.ainvoke({\"topic\": \"multi-agent systems\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83196d5",
   "metadata": {},
   "source": [
    "With asynchronous calls:\n",
    "- the chain does not block the main program,\n",
    "- multiple chains can run concurrently,\n",
    "- resources are used more efficiently.\n",
    "\n",
    "This mode is essential for scalable agent architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1822c068",
   "metadata": {},
   "source": [
    "## Your First \"Tool\" ‚Äî Building a Simple Function and a Runnable\n",
    "\n",
    "Up to now, our chains mostly produced *text*.\n",
    "This is useful, but agentic systems need more than text.\n",
    "\n",
    "Agents must be able to **do something** with model outputs:\n",
    "- transform them,\n",
    "- validate them,\n",
    "- extract structured signals,\n",
    "- call external systems (APIs, databases, search, etc.).\n",
    "\n",
    "Before introducing real tool calling, we start with the simplest idea:\n",
    "\n",
    "> a tool can be a plain Python function.\n",
    "\n",
    "In LangChain, a function can become a chain component by wrapping it as a Runnable.\n",
    "That is exactly what `RunnableLambda` is for.\n",
    "\n",
    "In this section, the function will be the \"tool\":\n",
    "it will take the LLM output and apply a deterministic transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc937e",
   "metadata": {},
   "source": [
    "To keep the example concrete, we will build a small chain that:\n",
    "1) asks the model to generate a short list,\n",
    "2) converts that text into a Python list,\n",
    "3) applies a function to post-process the result.\n",
    "\n",
    "This mirrors a common agent pattern:\n",
    "LLM output ‚Üí parsing/cleanup ‚Üí deterministic logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25332050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Give {n} short action ideas for an AI agent helping with {topic}. \"\n",
    "    \"Return them as a simple bullet list.\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213d794",
   "metadata": {},
   "source": [
    "Now we define a small Python function.\n",
    "This function acts like a ‚Äútool‚Äù because it is:\n",
    "- deterministic,\n",
    "- testable,\n",
    "- independent from the model.\n",
    "\n",
    "The goal here is simple: extract clean items from the bullet list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70b6b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bullets(text: str) -> list[str]:\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    items = []\n",
    "    for line in lines:\n",
    "        # remove common bullet markers\n",
    "        cleaned = line.lstrip(\"-‚Ä¢*0123456789. \").strip()\n",
    "        if cleaned:\n",
    "            items.append(cleaned)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd3d13",
   "metadata": {},
   "source": [
    "`RunnableLambda` turns this Python function into a runnable component.\n",
    "This makes it composable with LCEL, just like prompts or models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55f587bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bullet_extractor = RunnableLambda(extract_bullets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5af9ce",
   "metadata": {},
   "source": [
    "We can now build a complete pipeline:\n",
    "\n",
    "- the prompt formats the input,\n",
    "- the model generates text,\n",
    "- the parser extracts a string,\n",
    "- the function converts the string into a structured Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2830599",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model | output_parser | bullet_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b29b0",
   "metadata": {},
   "source": [
    "When the chain runs, the final output is no longer raw text.\n",
    "It becomes structured data that the rest of an agent system can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09bfc473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracted Actions (Python list) -----\n",
      "1. Automatically suggest optimal meeting times based on participants' availability and time zones.\n",
      "2. Send reminders and follow-up notifications for upcoming meetings and deadlines.\n",
      "3. Integrate with email to propose meeting times directly within email threads.\n",
      "4. Analyze past scheduling patterns to recommend the best days and times for recurring meetings.\n",
      "5. Automatically resolve scheduling conflicts by suggesting alternative times or notifying participants.\n"
     ]
    }
   ],
   "source": [
    "input_data = {\"n\": 5, \"topic\": \"calendar scheduling\"}\n",
    "actions = chain.invoke(input_data)\n",
    "\n",
    "print(\"----- Extracted Actions (Python list) -----\")\n",
    "for i, a in enumerate(actions, start=1):\n",
    "    print(f\"{i}. {a}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015570e7",
   "metadata": {},
   "source": [
    "This example is not a tool in the LangChain ‚ÄúTools API‚Äù sense yet.\n",
    "But it captures the core idea:\n",
    "\n",
    "- the LLM provides flexible generation,\n",
    "- Python provides reliable execution.\n",
    "\n",
    "Agentic systems live at the intersection of these two worlds.\n",
    "\n",
    "A real agent will use tools to:\n",
    "- search the web,\n",
    "- query a database,\n",
    "- call an API,\n",
    "- trigger workflows.\n",
    "\n",
    "The mental model remains the same:\n",
    "LLM output ‚Üí runnable logic ‚Üí next decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64deeb8d",
   "metadata": {},
   "source": [
    "This chapter introduced the foundations needed for agentic development:\n",
    "\n",
    "- composability with LCEL,\n",
    "- handling multiple inputs,\n",
    "- parallel execution,\n",
    "- different execution modes (stream, batch, async),\n",
    "- turning Python logic into runnable components.\n",
    "\n",
    "With these building blocks, we are ready to move from *chains*\n",
    "to real agent workflows.\n",
    "\n",
    "In the next chapter, We will use these same composable pieces,\n",
    "but we will organize them inside a graph-based loop:\n",
    "observe ‚Üí think ‚Üí act ‚Üí observe again.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
